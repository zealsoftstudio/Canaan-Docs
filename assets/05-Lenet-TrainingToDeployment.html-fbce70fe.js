import{_ as s,r as o,o as p,c as d,d as l,w as i,b as t,e as a,a as e}from"./app-21fd3c9b.js";const c={},r=a('<h1 id="lenet模型之从训练到端侧部署" tabindex="-1"><a class="header-anchor" href="#lenet模型之从训练到端侧部署" aria-hidden="true">#</a> Lenet模型之从训练到端侧部署</h1><h2 id="_1-前言" tabindex="-1"><a class="header-anchor" href="#_1-前言" aria-hidden="true">#</a> 1 前言</h2><h3 id="_1-1-读者对象" tabindex="-1"><a class="header-anchor" href="#_1-1-读者对象" aria-hidden="true">#</a> 1.1 读者对象</h3><p>本文档（本指南）主要适用于以下人员： • 技术支持工程师 • 软件开发工程师 • AI 应用案客户</p><h2 id="_2-lenet-模型简介" tabindex="-1"><a class="header-anchor" href="#_2-lenet-模型简介" aria-hidden="true">#</a> 2 Lenet 模型简介</h2><p>Lenet 是一系列网络的合称，包括Lenet1 - Lenet5，由Yann LeCun 等人在1990 年《Handwritten Digit Recognition with a Back-Propagation Network》中提</p><p>出，是卷积神经网络的HelloWorld。这里就以lenet 为例介绍AI model 在tina 平台上部署从训练到端侧运行的全部过程。</p><p>细分环节包括, 模型训练，模型导入，模型量化，模型推理，模型导出，模型仿真，模型profile，模型端侧部署几个部分. 用一幅图表示如下：</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185412148.png" alt="image-20221207185412148"></p>',9),u=e("p",null,"接下来从头开始介绍。",-1),_=e("h3",{id:"_2-1-模型训练",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-1-模型训练","aria-hidden":"true"},"#"),t(" 2.1 模型训练")],-1),h=e("p",null,"本例中使用keras 框架编写并训练lenet5 网络，训练完成后，导出h5 格式的模型文件,acuity tools 原生支持H5 格式.",-1),m=e("p",null,"模型结构：",-1),g=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185520650.png",alt:"image-20221207185520650"})],-1),v=e("p",null,"训练完成后，观察精度是否满足训练目标要求，本例精度达到了%97, 可以拿来部署说明问题:",-1),f=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185544323.png",alt:"image-20221207185544323"})],-1),b=e("p",null,"输出保存模型为lenet.h5,",-1),x=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185621907.png",alt:"image-20221207185621907"})],-1),A=e("p",null,"使用netron 查看模型结构:",-1),k=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185721712.png",alt:"image-20221207185721712"})],-1),I=e("p",null,"至此，我们的原生模型已经产生，接下来就可以进行PC 侧以及端侧的部署了，下一个环节是模型导入.",-1),w=e("h3",{id:"_2-2-模型导入",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-2-模型导入","aria-hidden":"true"},"#"),t(" 2.2 模型导入")],-1),P=e("p",null,"在进行导入操作前，先看一下部署目录的结构：",-1),y=e("p",null,"如下图所示，其中data 目录的图像来源于mnist 数据集，作用是用来作为后训练量化的数据输入，用于给量化算法提供输入参考，从而获知实际场景的数据输入分",-1),D=e("p",null,"布.dataset.txt 则是对data目录的引用，工具会通过dataset.txt 文件查找到data 目录中的每张图片。",-1),j=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185811861.png",alt:"image-20221207185811861"})],-1),N=e("p",null,"数据集：",-1),z=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-2022120718588762.png",alt:"image-2022120718588762"})],-1),L=a(`<h3 id="_2-3-导入模型" tabindex="-1"><a class="header-anchor" href="#_2-3-导入模型" aria-hidden="true">#</a> 2.3 导入模型</h3><p>使用芯原提供的acuity tool 中的pegasus 工具进行模型的导入.</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus import onnx --model yolact-sim.onnx --output-model yolact-sim.json --output-data yolact-sim.data
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>导入模型的目的是将开放模型转换为符合VIP 模型网络描述文件(.json) 和权重文件(.data)</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185921978.png" alt="image-20221207185921978"></p>`,5),q=a(`<div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus import keras --model lenet.h5 --output-data lenet.data --output-model lenet.json
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>执行成功后，可以看到目录中多了lenet.json 和lenet.data 文件，它们分别是符合芯原格式的模型结构文件和模型权重文件.</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207185955890.png" alt="image-20221207185955890"></p>`,3),U=e("p",null,"导入阶段，pegasus 工具也会对模型结构进行解析并输出：",-1),E=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190015642.png",alt:"image-20221207190015642"})],-1),B=a(`<p>导入阶段到这里还没有完，我们需要生成对网络的输入输出描述文件，这也是acuity tool 工具要求的，输入输出描述是YML 格式的文本文件，后面我们将通过修</p><p>改YML 文件来对模型参数，输入/输出tensor 格式等信息进行配置.</p><h4 id="_2-3-1-创建input-output-yml-文件" tabindex="-1"><a class="header-anchor" href="#_2-3-1-创建input-output-yml-文件" aria-hidden="true">#</a> 2.3.1 创建input/output YML 文件</h4><p>YML 文件对网络的输入和输出进行描述，比如输入图像的形状，归一化系数(均值，零点)，图像格式，输出tensor 的输出格式，后处理方式等等，命令如下：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus generate inputmeta --model lenet.json --input-meta-output lenet-inputmeta.yml pegasus
generate postprocess-file --model lenet.json --postprocess-file-output lenet-postprocess-file.yml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div></div></div><p>命令成功执行后，目录中又多了两个文件，分别是input 和output YML 文件。</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190103043.png" alt="image-20221207190103043"></p>`,7),V=e("p",null,"至此，模型导入阶段的工作才算全部完成，从这里开始，模型已经被转成了芯原格式的模型文件。后续步骤已经和原生模型没有太大关系了.",-1),S=e("p",null,"在执行下一步的模型量化前，我们需要修改input yml 文件的scale，mean 参数，使其和训练时的参数保持一致.",-1),T=e("p",null,"训练代码中的均值和scale 为：",-1),K=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190126855.png",alt:"image-20221207190126855"})],-1),M=e("p",null,"根据代码，均值mean = 0, scale 为1/255 = 0.0039.",-1),Y=e("p",null,"修改YML 为对应值：",-1),C=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190147405.png",alt:"image-20221207190147405"})],-1),R=a(`<h3 id="_2-4-模型量化" tabindex="-1"><a class="header-anchor" href="#_2-4-模型量化" aria-hidden="true">#</a> 2.4 模型量化</h3><p>此款NPU 支持uint8,int8,int16 三种量化类型，基于推理速度和精度折衷的考虑，我们用int8，非对称量化模式，量化命令如下：</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus quantize --model lenet.json --model-data lenet.data --batch-size 1 --device CPU --withinput-meta lenet-inputmeta.yml --rebuild --model-quantize lenet.quantilize --quantizer asymmetric_affine --qtype uint8
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>命令执行后，工程目录下可以看到新创建的量化表文件文件</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190243780.png" alt="image-20221207190243780"></p>`,5),F=a(`<h3 id="_2-5-模型预推理" tabindex="-1"><a class="header-anchor" href="#_2-5-模型预推理" aria-hidden="true">#</a> 2.5 模型预推理</h3><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus inference --model lenet.json --model-data lenet.data --batch-size 1 --dtype quantized --model-quantize lenet.quantilize --device CPU --with-input-meta lenet-inputmeta.yml --postprocess-file lenet-postprocess-file.yml --iterations 10
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>参数中，--batch-size 1表示每次推理处理1 张图像，我们dataset 目录中包含了10 张图像，所以要运行10 次处理完毕，这也是后面参数--iterations 10的作用.</p><p>成功运行后，目录中多出了20 个文本格式的.tensor 文件，文件中保存的是每张图像的输入和输出的tensor 数据，默认情况下，只对输入输出tensor 进行保存，</p><p>你可以通过下面命令将每层的tensor 都保存下来:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus dump --model lenet.json --model-data lenet.data --with-input-meta lenet-inputmeta.yml
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>推理结束后创建的tensor 文件：</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221207190401783.png" alt="image-20221207190401783"></p><p>我们从命令的输出来看，也可以看出推理是否正确, 我们以前六个为例，可以看到top5 输出中，每次概率最高的分别是0,1,2,3,4 ….，和我们dataset.txt 文件实际</p><p>输入的图像数据顺序是相符的。</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208090925317.png" alt="image-20221208090925317"></p>`,11),G=e("p",null,"输出tensor 则为最后一层的softmax 输出，也就是分别为数字0-9 的概率，以第9 张图像的输出tensor 为例，概率最大的是9, 如下图：",-1),O=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208090956262.png",alt:"image-20221208090956262"})],-1),H=e("p",null,"输入则是输入图像正则化后的浮点数据：",-1),X=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091015981.png",alt:"image-20221208091015981"})],-1),$=a(`<p>至此模型预推理阶段结束，进入下一步模型导出阶段.</p><h3 id="_2-6-导出代码和nbg-文件" tabindex="-1"><a class="header-anchor" href="#_2-6-导出代码和nbg-文件" aria-hidden="true">#</a> 2.6 导出代码和NBG 文件</h3><p>导出代码的命令如下，两次命令的区别只有选项--pack-nbg-unify和--pack-nbg-viplite，其余完全相同。其中--pack-nbg-unify生成的是仿真侧的代码，而--pack-</p><p>nbg-viplite则会生成在端侧运行部署的代码，两条命令分别执行一遍。</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus export ovxlib --model lenet.json --model-data lenet.data --dtype quantized --model-quantize
lenet.quantilize --batch-size 1 --save-fused-graph --target-ide-project &#39;linux64&#39; --with-inputmeta
lenet-inputmeta.yml --postprocess-file lenet-postprocess-file.yml --output-path ovxlib/lenet/
lenet --pack-nbg-unify --optimize &quot;VIP9000PICO_PID0XEE&quot; --viv-sdk \${VIV_SDK}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>pegasus export ovxlib --model lenet.json --model-data lenet.data --dtype quantized --model-quantize
lenet.quantilize --batch-size 1 --save-fused-graph --target-ide-project &#39;linux64&#39; --with-inputmeta
lenet-inputmeta.yml --postprocess-file lenet-postprocess-file.yml --output-path ovxlib/lenet/
lenet --pack-nbg-viplite --optimize &quot;VIP9000PICO_PID0XEE&quot; --viv-sdk \${VIV_SDK}
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div><div class="line-number"></div></div></div><p>执行结束后，工程代码和NBG 文件都已经生成了:</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091107463.png" alt="image-20221208091107463"></p>`,8),W=a(`<p>导出的NBG 文件可以投入到NPU 中运行.</p><p>ovxlib/lenet/工程用于仿真和profile.</p><p>ovxlib/lenet_nbg_viplite/可以用部署在端侧运行.</p><p>ovxlib/lenet_nbg_unify 和ovxlib/lenet_nbg_unify_ovx 暂时可以不用理会，没有用处。</p><p>接下来，进入模型仿真环节.</p><h3 id="_2-7-模型仿真" tabindex="-1"><a class="header-anchor" href="#_2-7-模型仿真" aria-hidden="true">#</a> 2.7 模型仿真</h3><p>上文说到，ovxlib/lenet/是用于仿真和profile 分析的工程，下面我们启动IDE 运行仿真.</p><h4 id="_2-7-1-启动ide" tabindex="-1"><a class="header-anchor" href="#_2-7-1-启动ide" aria-hidden="true">#</a> 2.7.1 启动IDE</h4><p>启动IDE 的命令如下:</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>~/VeriSilicon/VivanteIDE5.5.0/ide/vivanteide5.5.0
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>启动时，首先选择一个工作目录用于保存仿真工程：</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091310996.png" alt="image-20221208091310996"></p>`,12),J=e("h4",{id:"_2-7-2-导入ovxlib-lenet-工程",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-7-2-导入ovxlib-lenet-工程","aria-hidden":"true"},"#"),t(" 2.7.2 导入ovxlib/lenet 工程")],-1),Q=e("p",null,"IDE 中，选择File->Import->General选项卡->Existing Projects into Workspace",-1),Z=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091730108.png",alt:"image-20221208091730108"})],-1),ee=e("p",null,"之后选择模型导出阶段创建的工程目录ovxlib/lenet/",-1),te=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091748054.png",alt:"image-20221208091748054"})],-1),ne=e("p",null,"这里强烈建议选中Copy projects into workspace, 这样我们的仿真工程将会拷贝一份到IDE 工作空间中，保证与导出空间隔离.",-1),le=e("p",null,"之后点击Finsih，结束导入过程. 导入后的工作空间如下所示：",-1),ie=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091811642.png",alt:"image-20221208091811642"})],-1),ae=e("h4",{id:"_2-7-3-编译工程",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-7-3-编译工程","aria-hidden":"true"},"#"),t(" 2.7.3 编译工程")],-1),se=e("p",null,"执行菜单命令Project->Build All，先将仿真工程编译一下，看有没有犯低级错误(比如导错工程了等等.):",-1),oe=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091838854.png",alt:"image-20221208091838854"})],-1),pe=e("p",null,"编译OK，生成了可执行lenet 文件，我们进入下一步.",-1),de=e("h4",{id:"_2-7-4-配置仿真参数",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-7-4-配置仿真参数","aria-hidden":"true"},"#"),t(" 2.7.4 配置仿真参数")],-1),ce=e("p",null,"执行菜单命令Run->Debug Configurations...在选项卡中，双击OpenVX Application，即可出现下图的输出，默认情况下Search Project 和Browser 按钮窗口会被",-1),re=e("p",null,"正确设置成如下图的样子，如果没有，请按照上面编译的结果正确选择工程和应用路径。",-1),ue=e("p",null,"这个选项卡最最重要的设置是Program arguments，不同的网络根据输入输出个数的不同，输入也不尽相同，lenet 按照如下的方法设置:",-1),_e=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091913430.png",alt:"image-20221208091913430"})],-1),he=e("p",null,"其中lenet.export.data 是量化权重，它是在模型导出阶段生成在ovxlib/lenet 工程中的，工程导入阶段已经自动拷贝到IDE 仿真工程下，不需要手工拷贝，而0.jpg",-1),me=e("p",null,"则需要手动拷贝到IDE工程下:",-1),ge=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208091937601.png",alt:"image-20221208091937601"})],-1),ve=e("p",null,"输入除了图片，也可以是模型推理阶段生成的输入tensor, 仿真程序会根据后缀名自动运行到不同的处理分支，保证处理结果都是对的。",-1),fe=e("p",null,"点击Apply, 之后就可以开始正式仿真了.",-1),be=e("h4",{id:"_2-7-5-仿真",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-7-5-仿真","aria-hidden":"true"},"#"),t(" 2.7.5 仿真")],-1),xe=e("p",null,"点击工具栏Run 按钮，弹出对话框，直接点击Run 触发仿真.",-1),Ae=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092001120.png",alt:"image-20221208092001120"})],-1),ke=e("p",null,"lenet 是很小的网络，仿真很快结束，输出如下, 根据下面控制台的输出可以看到，我们给的参数图像是0.jpg，而仿真输出的top5 结果表明，推理结果为0 的概率",-1),Ie=e("p",null,"为%99.0023, 符合预期.",-1),we=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092059915.png",alt:"image-20221208092059915"})],-1),Pe=e("p",null,"至此，模型仿真结束，进入模型profile.",-1),ye=e("h3",{id:"_2-8-模型profile",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-8-模型profile","aria-hidden":"true"},"#"),t(" 2.8 模型Profile")],-1),De=e("p",null,"模型profile 可以帮助分析网络的整体运行效率，带宽，帧率以及各层的处理性能，是分析算法精度，性能瓶颈等问题的利器。",-1),je=e("p",null,"点击工具栏运行旁边的Profile按钮即可触发Profile 操作，同样在选项卡中选择Profile 按钮继续.",-1),Ne=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092134981.png",alt:"image-20221208092134981"})],-1),ze=e("p",null,"之后点击Resume 继续:",-1),Le=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092154631.png",alt:"image-20221208092154631"})],-1),qe=e("p",null,"Profile 结束后, IDE 输出如下：",-1),Ue=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092325476.png",alt:"image-20221208092325476"})],-1),Ee=e("p",null,"简单分析一下Profile 的信息，左下角和仿真结果输出相同，为推理top5 的结果，中间的是各层的运行情况统计，包括每层的硬件处理单元，读写带宽，对于由PPU 计算的层，比如softmax层，还会有指令数统计等等。右下角的则是网络的整体运行性能分析，包括网络整体的读写带宽，处理帧率，时钟数，等等信息。更具体地分析，请参考芯原文档。",-1),Be=e("p",null,"接下来，我们到了最后一步，我们前面所作地一切工作地目的，就是要将网络部署在端侧，真正地在板子上跑起来。我们开始端侧部署地介绍:",-1),Ve=e("h3",{id:"_2-9-端侧部署",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-9-端侧部署","aria-hidden":"true"},"#"),t(" 2.9 端侧部署")],-1),Se=e("p",null,"前面仿真阶段，仿真结束后，工程中生成了两个bin 文件是我们部署验证要用到的，分别是input_0.dat 和output0_10_1.dat, 它们都是二进制格式的文件。",-1),Te=e("p",null,"input_0.dat 是网络第一层的输入，output0_10_1.dat 是网络最后一层的输出，由于根据仿真结果说明，这两笔数据都是正确的，可以作为golden 数据和端侧的",-1),Ke=e("p",null,"运行结果进行对比，如果在同样的input 下，端侧跑出的output tensor 和output_0._10_1.dat 是binary identical 的，那就说明，端侧部署是正确的。",-1),Me=e("p",null,"理清了逻辑，我们开始动手操作，首先在仿真工程下认识一下这两个.dat, 下图左框中的蓝底文件:",-1),Ye=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092431537.png",alt:"image-20221208092431537"})],-1),Ce=e("p",null,"接下来，我们用到模型导出阶段生成的另一个工程，ovxlib/lenet_nbg_viplite 工程.",-1),Re=e("h4",{id:"_2-9-1-交叉编译ovxlib-lenet-nbg-viplite-工程",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-9-1-交叉编译ovxlib-lenet-nbg-viplite-工程","aria-hidden":"true"},"#"),t(" 2.9.1 交叉编译ovxlib/lenet_nbg_viplite 工程")],-1),Fe=e("p",null,"我们发布的Tina SDK 将会包含NPU 的开发SDK，NPU 的开发SDK 结构如下图所示：",-1),Ge=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092502665.png",alt:"image-20221208092502665"})],-1),Oe=e("p",null,"如果您拿到了tina sdk，NPU 的KMD 模块已经存在于tina sdk 对应的linux 内核代码中，而用户态SDK, 也就是图中的npu runtime library(UMD Driver)，则在SDK 的package/allwinner/目录下以库的形式存在。",-1),He=e("p",null,"接上文，我们将ovxlib/lenet_nbg_viplite 拷贝出来，和Tina 中NPU runtime library 并列放在一个目录下，按照下面的内容编写makefile 文件（SDK 中将会包含一个demo makefile).",-1),Xe=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092527322.png",alt:"image-20221208092527322"})],-1),$e=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092609567.png",alt:"image-20221208092609567"})],-1),We=e("p",null,"工程目录中，sdk 目录内容是NPU 的用户态运行库，也就是UMD 驱动，而lenet_nbg_viplite目录中的内容，则是模型部署阶段产生的lenet_nbg_viplite 工程加上makefile 的结果。",-1),Je=e("p",null,"接下来就可以交叉编译测试工程了，在lenet_nbg_viplite 目录，执行make clean && make ,执行结束后，在out 目录将会生成可以在端侧跑的lenet 测试程序。",-1),Qe=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092632515.png",alt:"image-20221208092632515"})],-1),Ze=e("p",null,"交叉编译到此结束，接下来准备测试工程目录。",-1),et=e("h4",{id:"_2-9-2-准备测试工程目录",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-9-2-准备测试工程目录","aria-hidden":"true"},"#"),t(" 2.9.2 准备测试工程目录")],-1),tt=e("p",null,"测试工程目录的内容包括，模型NBG 文件，lenet 可执行程序，两个UMD 动态库以及仿真阶段生成的input_0.dat，一共5 个文件，如下图所示：",-1),nt=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092713183.png",alt:"image-20221208092713183"})],-1),lt=e("p",null,"至此，测试目录已准备OK，下面开始准备端侧验证平台。",-1),it=e("h4",{id:"_2-9-3-准备端侧验证环境",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-9-3-准备端侧验证环境","aria-hidden":"true"},"#"),t(" 2.9.3 准备端侧验证环境")],-1),at=e("p",null,"首先，NPU 运行的大是你的端侧存下下面的设备节点/dev/vipcore，否则，说明SDK 配置是错误的，请寻求我们的协助。",-1),st=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092735339.png",alt:"image-20221208092735339"})],-1),ot=a(`<p>将前面建立的测试目录保存到tf 卡中，我们用TF 卡作为媒介验证, 将TF 卡插到端侧平台，之后执行命令</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>mount -t vfat /dev/mmcblkxxx /mnt/sdcard
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>将其挂载到/mnt/sdcard 目录，之后，进入lenet-test 验证目录，执行命令</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:/mnt/sdcard/lenet-test
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>保证运行库可以被正确的连接到。</p><p>之后就可以正式测试了，执行测试命令</p><div class="language-text line-numbers-mode" data-ext="text"><pre class="language-text"><code>./lenet network_binary.nb input_0.dat
</code></pre><div class="line-numbers" aria-hidden="true"><div class="line-number"></div></div></div><p>输出如下：</p><p><img src="http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092904086.png" alt="image-20221208092904086"></p>`,9),pt=e("p",null,"注意最后一行，可以看到测试目录下多了一个文件，output0_10_1.dat，他就是网络输出的结果。",-1),dt=e("h3",{id:"_2-10-验证",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-10-验证","aria-hidden":"true"},"#"),t(" 2.10 验证")],-1),ct=e("p",null,"我们得到了tensor 的端侧运行结果，将他和仿真生成的.dat 做对比，预期情况应该是binary identical 的。",-1),rt=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208092937875.png",alt:"image-20221208092937875"})],-1),ut=e("p",null,"结果Binary Identical 的，和我们的预期一致.",-1),_t=e("h4",{id:"_2-10-1-验证tensor",tabindex:"-1"},[e("a",{class:"header-anchor",href:"#_2-10-1-验证tensor","aria-hidden":"true"},"#"),t(" 2.10.1 验证tensor")],-1),ht=e("p",null,"根据前面网络结构的描述，网络的最后一层是softmax,softmax 层输出概率值为浮点数，在芯原的NPU 设计中，存在三类计算单元，分别是TP,NNE 和PPU，这里",-1),mt=e("p",null,"面只有PPU 支持浮点计算，所以softmax 层要在PPU 上运行的。",-1),gt=e("p",null,"我们看一下NBG 文件中的输出层信息, 如下图所示，可以看到输出为浮点FP16，没有量化，根据上面验证beyondcompare 对比，我们也可以看出，tensor 输出",-1),vt=e("p",null,"一共20 个字节。我们来计算一下，lenet 分类网络一共识别十类目标，每一类的概率为FP16，所以计算起来正好是20 个字节，由此我们知道了output tensor 的",-1),ft=e("p",null,"结构。",-1),bt=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208093034585.png",alt:"image-20221208093034585"})],-1),xt=e("p",null,"既然知道了结构，我们就可以将运行产生的output tensor 转换为概率打印出来，由于32 位机上不支持FP16 的格式，所以需要将其转换为符合ieee754 的float32",-1),At=e("p",null,"格式，核心转换代码如下:",-1),kt=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208093112338.png",alt:"image-20221208093112338"})],-1),It=e("p",null,"输出如下：",-1),wt=e("p",null,[e("img",{src:"http://photos.100ask.net/allwinner-docs/v853/AIApplication/image-20221208093127325.png",alt:"image-20221208093127325"})],-1),Pt=e("p",null,"对比仿真阶段的输出，得到的TOP5 输出概率完全一样。说明我们的部署以及后处理是正确的。",-1),yt=e("p",null,"至此，部署加验证过程全部结束~！",-1);function Dt(jt,Nt){const n=o("center");return p(),d("div",null,[r,l(n,null,{default:i(()=>[t("图3-1: npu 部署流程")]),_:1}),u,_,h,m,g,l(n,null,{default:i(()=>[t("图3-2: lenet5 模型结构")]),_:1}),v,f,b,x,l(n,null,{default:i(()=>[t("图3-4: lenet 模型文件")]),_:1}),A,k,l(n,null,{default:i(()=>[t("图3-5: lenet 模型结构查看")]),_:1}),I,w,P,y,D,j,l(n,null,{default:i(()=>[t("图3-6: 部署目录结构")]),_:1}),N,z,l(n,null,{default:i(()=>[t("图3-7: 量化参考图像数据集")]),_:1}),L,l(n,null,{default:i(()=>[t("图3-8: npu_import")]),_:1}),t(" 接下来进行lenet.h5 模型导入. "),q,l(n,null,{default:i(()=>[t("图3-9: 模型导入")]),_:1}),U,E,l(n,null,{default:i(()=>[t("图3-10: 模型结构描述")]),_:1}),B,l(n,null,{default:i(()=>[t("图3-11: YML 文件生成")]),_:1}),V,S,T,K,l(n,null,{default:i(()=>[t("图3-12: 训练代码中的均值和scale")]),_:1}),M,Y,C,l(n,null,{default:i(()=>[t("图3-13: 修改input YML Scale 参数")]),_:1}),R,l(n,null,{default:i(()=>[t("图2-14: 量化表文件")]),_:1}),F,l(n,null,{default:i(()=>[t("图2-16: 部署推理过程输出")]),_:1}),G,O,l(n,null,{default:i(()=>[t("图2-17: softmax 输出")]),_:1}),H,X,l(n,null,{default:i(()=>[t("图2-18: 输入tensor")]),_:1}),$,l(n,null,{default:i(()=>[t("图2-19: 导出模型和工程代码")]),_:1}),W,l(n,null,{default:i(()=>[t("图2-20: IDE 仿真")]),_:1}),J,Q,Z,l(n,null,{default:i(()=>[t("图2-21: npu_prj_import")]),_:1}),ee,te,l(n,null,{default:i(()=>[t("图2-22: npu_prj_ok")]),_:1}),ne,le,ie,l(n,null,{default:i(()=>[t("图2-23: npu_sim_ok")]),_:1}),ae,se,oe,l(n,null,{default:i(()=>[t("图2-24: npu_compile_ok")]),_:1}),pe,de,ce,re,ue,_e,l(n,null,{default:i(()=>[t("图2-25: npu_argu")]),_:1}),he,me,ge,l(n,null,{default:i(()=>[t("图2-26: npu_arg_copy")]),_:1}),ve,fe,be,xe,Ae,l(n,null,{default:i(()=>[t("图2-27: npu_run")]),_:1}),ke,Ie,we,l(n,null,{default:i(()=>[t("图2-28: npu_sim_res")]),_:1}),Pe,ye,De,je,Ne,l(n,null,{default:i(()=>[t("图2-29: npu_profile_con")]),_:1}),ze,Le,l(n,null,{default:i(()=>[t("图2-30: npu_profile_in")]),_:1}),qe,Ue,l(n,null,{default:i(()=>[t("图2-31: npu_lenet_profile")]),_:1}),Ee,Be,Ve,Se,Te,Ke,Me,Ye,l(n,null,{default:i(()=>[t("图2-32: goldentensor")]),_:1}),Ce,Re,Fe,Ge,l(n,null,{default:i(()=>[t("图2-33: npu sdk")]),_:1}),Oe,He,Xe,l(n,null,{default:i(()=>[t("图2-34: 工程目录结构")]),_:1}),$e,l(n,null,{default:i(()=>[t("图2-35: 工程makefile")]),_:1}),We,Je,Qe,l(n,null,{default:i(()=>[t("图2-36: npu_elf_res")]),_:1}),Ze,et,tt,nt,l(n,null,{default:i(()=>[t("图2-37: npu_test_env")]),_:1}),lt,it,at,st,l(n,null,{default:i(()=>[t("图2-38: npu_device")]),_:1}),ot,l(n,null,{default:i(()=>[t("图2-39: result")]),_:1}),pt,dt,ct,rt,l(n,null,{default:i(()=>[t("图2-40: npu_cmp_res")]),_:1}),ut,_t,ht,mt,gt,vt,ft,bt,l(n,null,{default:i(()=>[t("图2-41: nbinfo")]),_:1}),xt,At,kt,l(n,null,{default:i(()=>[t("图2-42: npu_fp16")]),_:1}),It,wt,l(n,null,{default:i(()=>[t("图2-43: npu_fp32")]),_:1}),Pt,yt])}const Lt=s(c,[["render",Dt],["__file","05-Lenet-TrainingToDeployment.html.vue"]]);export{Lt as default};
